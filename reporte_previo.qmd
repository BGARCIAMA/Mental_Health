---
title: "Mental Health"
author: 
  - "Lauro Reyes Rosas - 214532"
  - "Blanca Estela García Manjarrez - 118886"
  - "Yuneri Pérez Arellano - 199813"
format:
  html:
    code-fold: false
jupyter: python3
subtitle: "ITAM - Aprendizaje de Máquina"
date: "Fecha de entrega: 2024-12-11"
professor: "Felipe González"
cache: true
---

# Predicción de Depresión en Adultos

### Un enfoque con machine learning

Este proyecto es de carácter educativo y no plantea predicciones como solución al problema de salud mental de depresión, la información e iniciativa del proyecto se atribuye al concurso de Kaggle del tema Salud Mental.

## 1. Definición del proyecto

### 1.1. Introducción

La depresión es un trastorno del estado de ánimo que afecta cómo una persona se siente, piensa y maneja las actividades diarias, como dormir, comer o trabajar. Va más allá de sentirse triste o desanimado ocasionalmente; es una condición persistente que puede durar semanas, meses o incluso años.

Algunos síntomas comunes de la depresión van desde los emocionales hasta los físicos y cognitivos, y pueden variar en intensidad de una persona a otra. Algunos de los síntomas más comunes incluyen:

-   **Emocionales:**
    -   Tristeza profunda o sensación de vacío.
    -   Pérdida de interés o placer en actividades que solían disfrutarse.
    -   Sentimientos de culpa, inutilidad o desesperanza.
-   **Físicos**:
    -   Cambios en el apetito o el peso.
    -   Problemas de sueño (insomnio o dormir en exceso).
    -   Fatiga constante o falta de energía.
-   **Cognitivos**:
    -   Dificultad para concentrarse, pensar o tomar decisiones.
    -   Pensamientos recurrentes de muerte o suicidio.

Según la OMS (Organización Mundial de la Salud), la depresión es una de las principales causas de discapacidad en todo el mundo y afecta a personas de todas las edades. Aproximadamente el 5% de los adultos en todo el mundo padecen depresión, lo que equivale a cerca de 280 millones de personas. La depresión es más frecuente en mujeres (6%) que en hombres (4%). En 2020, se estimó que más de 264 millones de personas en todo el mundo padecían depresión, lo que representa un aumento del 18% en la última década. La depresión puede tener un impacto significativo en la calidad de vida de las personas, afectando su bienestar emocional, social y físico.

### 1.2. Descripción general del proyecto

En este proyecto, abordaremos la tarea de clasificación, es decir, tener **Depresión** o no, a partir de variables presentes en el conjunto de datos titulado [**Salud Mental**](https://www.kaggle.com/competitions/playground-series-s4e11/data). Este dataset fue extraído de Kaggle, mismo que fue el resultado de una encuesta integral *cuyo propósito fue analizar los factores asociados al riesgo de depresión en adultos.* La encuesta incluyó preguntas sobre diversos aspectos de la vida de los participantes, como presión académica, satisfacción laboral, hábitos de sueño y alimentación, entre otros.

La encuesta fue llevada a cabo de manera anónima entre enero y junio de 2023 en diversas ciudades, involucrando a personas de diferentes orígenes y profesiones.

Los participantes, con edades entre 18 y 60 años, compartieron voluntariamente información sobre una amplia variedad de aspectos, tales como edad, género, ciudad de residencia, entre otros.

Cabe destacar, que se asume correcta lo reportado en el data set de Kaggle sobre si el adulto tiene o no depresión.

### 1.3. Objetivos

El análisis de este conjunto de datos, permitirá explorar y modelar las relaciones entre la sabana analítica y la variable objetivo.

Los objetivos específicos de este proyecto son:

1.  Realizar un análisis exploratorio de los datos para comprender la distribución y relaciones entre las variables.
2.  Realizar ingeniería de características para mejorar la representación de los datos y el desempeño del modelo.
3.  Entrenar un modelo de machine learning para clasificar si un adulto con ciertas características tiene depresión o no, con base en lo estipulado en el concurso de Kaggle.
4.  Evaluar el desempeño del modelo.

### 1.4. Métricas de Evaluación

Para evaluar el desempeño del modelo, utilizaremos la métrica de precisión (*accuracy*) como lo plantea el concurso de Kaggle. Con ella se medirá la proporción de clasificaciones correctas en relación con el total.

Dado que la variable objetivo está desbalanceada, la precisión proporcionará una medida general del rendimiento del modelo, considerando tanto los casos positivos como los negativos.

## 2. Análisis exploratorio de datos (EDA)

Es esta sección, se presentará el resultado del análisis exploratorio de los datos.

### 2.1. Características del dataset

El dataset está compuesto por 20 variables, descritas a continuación:

| Variable | Descripción |
|------------------------------|------------------------------------------|
| `id` | Identificador único del registro |
| `Nombre` | Nombre del participante |
| `Género` | Género del participante |
| `Edad` | Edad del participante |
| `Ciudad` | Ciudad de residencia |
| `Profesionista/Estudiante` | Ocupación principal |
| `Profesión` | Campo laboral o académico |
| `Presión_académica` | Nivel de presión relacionado con los estudios |
| `Presión_laboral` | Nivel de presión relacionado con el trabajo |
| `CGPA` | Promedio general acumulado |
| `Satisfacción_académica` | Nivel de satisfacción con los estudios |
| `Satisfacción_laboral` | Nivel de satisfacción con el trabajo |
| `Tiempo_dormir` | Horas promedio de sueño por día |
| `Hábitos_alimenticios` | Calidad percibida de los hábitos alimenticios |
| `Grado_académico` | Nivel más alto de educación alcanzado |
| `Pensamientos_suicidas` | Indicadores de pensamientos suicidas |
| `Horas_estudio/trabajo` | Promedio de horas dedicadas a estudio o trabajo |
| `Estrés_financiero` | Percepción de presión financiera |
| `Antecedente_familiar_enfermedad_mental` | Presencia de antecedentes familiares de enfermedad mental |
| `Depresión` | Indicador binario de depresión (**variable objetivo**) |

```{python}
#| echo: false
#| label: librerias
import numpy as np
import pandas as pd 
import matplotlib.pylab as plt 
import seaborn as sns 
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

sns.set_palette("pastel")

import warnings
warnings.filterwarnings('ignore')
```

### 2.2. Shape de los datasets

```{python}
#| echo: false
#| label: info
train = pd.read_csv('data/train.csv')
test = pd.read_csv("data/test.csv")
train.columns = [col.lower().replace(" ","_") for col in train.columns]
test.columns = [col.lower().replace(" ","_") for col in test.columns]
```

Los datasets de train y test tienen la siguiente estructura:

| Dataset |  Rows   | Columns |
|:-------:|:-------:|:-------:|
|  Train  | 140,700 |   20    |
|  Test   | 93,800  |   19    |

Cabe destacar que el conjunto de test, no incluye la variable objetivo para la clasificación (`Depression`).

### 2.3. Variables y tipo de datos

**Variables numéricas**

```{python}
#| echo: false
train.describe().round(2).style.format(precision=2)
```

-   El análisis exploratorio de las variables numéricas revela que las relacionadas con satisfacción, presión y estrés están en una escala de 1 a 5, con medias cercanas a 3.
-   La edad de los participantes oscila entre 18 y 60 años, con una media de 40.4 años, mientras que las horas de trabajo/estudio tienen una media de 6.25 horas diarias.
-   El rendimiento académico (`CGPA`) muestra valores altos, con una media de 7.66. La variable objetivo, `Depression`, indica que alrededor del 18% de los participantes tiene depresión.

**Variables categóricas**

```{python}
#| echo: false
train.describe(include="object")
```

-   En cuanto a las variables categóricas, destacan 98 ciudades únicas y 64 profesiones, con "Teacher" como la más común.
-   Respecto a la duración del sueño, se observa que "Less than 5 hours" son las más reportadas, mientras que los antecedentes familiares de enfermedad mental y pensamientos suicidas tienen predominancia de respuestas negativas.
-   Estos resultados iniciales subrayan la diversidad del dataset y señalan áreas clave para el tratamiento de datos, como los valores nulos y categorías con alta cardinalidad.

### 2.4. Descripción de variable objetivo

```{python}
#| echo: false
#| label: target variable dist
target_colors = [
    "#5EDB92",
    "#5A665F",
]
plt.figure(figsize=(6, 6))
plt.pie(
    train["depression"].value_counts(),
    labels=train["depression"].value_counts().index,
    textprops={"fontsize": 15, "color": "black"},
    colors=target_colors,
    autopct="%.0f%%",
    explode=[0.03, 0.03],
)
plt.title("Distribución de la variable Depresión", fontsize=14)
plt.show()
```

-   El análisis de la variable objetivo `depression` revela un conjunto de datos desbalanceado, donde aproximadamente el 82% de los registros corresponden a individuos sin depresión y solo el 18% a casos positivos. Esta desigualdad es visualmente evidente en la gráfica de distribución.

-   El desbalance en la variable objetivo establece un desafío en el modelado, ya que los algorítmos pueden inclinarse hacia la clase mayoritaria. Sin embargo, también sugiere que un punto de referencia inicial para la métrica de precisión (*accuracy*) debe superar el 82%, ya que ese sería el desempeño esperado al clasificar siempre la clase mayoritaria.

### 2.5. Valores nulos

```{python}
#| echo: false
#| label: NA check

null_percentage = (train.isnull().sum() / len(train)) * 100
non_null_percentage = 100 - null_percentage
percentage_df = pd.DataFrame({
    'Valores Nulos (%)': null_percentage,
    'Valores Presentes (%)': non_null_percentage
})
# Filtrar solo columnas con valores nulos
percentage_df = percentage_df[null_percentage > 0]
# Gráfico apilado
percentage_df.plot(kind='bar', stacked=True, figsize=(8, 6), color=['red', 'green'])
plt.title("Porcentaje de Valores Nulos")
plt.ylabel("Porcentaje (%)")
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.show()
```

-   En particular, las variables relacionadas con estudiantes, como `academic_pressure`, `cgpa`, y `study_satisfaction`, presentan hasta un 80% de valores nulos. Este resultado refleja una menor representación de estudiantes en comparación con profesionistas dentro del dataset.

-   A pesar del alto porcentaje de valores faltantes en estas variables, no se eliminarán del análisis, ya que contienen información valiosa para los casos específicos de estudiantes. Este enfoque permite preservar datos que pueden ser relevantes en el desarrollo del modelo, especialmente para evaluar las diferencias entre grupos ocupacionales.

-   Las estrategias de imputación y tratamiento de valores nulos serán fundamentales para garantizar la calidad del análisis posterior.

### 2.6. Visualizaciones

En esta sección, se presentará el análisis visual de las variables del dataset con ayuda de gráficos.

```{python}
#| echo: false
#| label: librerias_visualizaciones

import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
import squarify
```

```{python}
#| echo: false
num_feats = train.select_dtypes(include="float64").columns
obj_feats = train.select_dtypes(include="object").columns
```

<!-- -->

```{python}
#| echo: false
cat_colors = ['#1c76b6', '#a7dae9', '#eb6a20', '#f59d3d', '#677fa0', '#d6e4ed', '#f7e9e5']

cat_to_plot = ['gender', 'working_professional_or_student', 'academic_pressure', 
                       'work_pressure', 'study_satisfaction', 'job_satisfaction',
                       'have_you_ever_had_suicidal_thoughts_?', 'financial_stress', 
                       'family_history_of_mental_illness']
for column in cat_to_plot:
    plt.figure(figsize=(5, 3))
    sns.countplot(data=train, x=column, palette=cat_colors)
    plt.title(f'Diagrama de recuento de {column}')
    plt.tight_layout()
    plt.show()
```

-   El dataset, muestra una mayor representación de hombres en comparación con mujeres, con una significativa mayoría perteneciente a la categoría de profesionales que trabajan.

-   Los hábitos alimenticios de las personas son diversos, distribuidos entre categorías moderadas, saludables y no saludables de manera equilibrada.

-   En cuanto a los patrones de sueño, la mayoría de los individuos reportan dormir entre 5 y 8 horas por noche.

-   Para las variables como `academic_pressure`, `work_pressure`, `study_satisfaction`, `job_satisfaction` y `financial_stress` las distribuciones son multimodales, lo que nos indica que están influenciadas por factores externos que generan diferentes "grupos" y las consideraremos como categóricas, asumiendo que en la encuesta se manejó escala del 1 al 5.

-   Una proporción significativa de la población reporta haber tenido pensamientos suicidas, con una distribución casi equilibrada entre respuestas afirmativas y negativas.

-   Además, aproximadamente la mitad de las personas indican antecedentes familiares de enfermedades mentales. Estas estadísticas resaltan la importancia de investigar más a fondo los factores asociados con la salud mental, desde el punto de vista acádemico de este proyecto.

```{python}
#| echo: false
bi_palette = ["#5EDB92",
              "#5A665F"]
num_to_plot = ["age", "cgpa", "work/study_hours"]
target = train.select_dtypes(include="int64").columns

for column in num_to_plot:
    plt.figure(figsize=(5, 3))
    sns.violinplot(data = train, x = target[1], y = column, palette = bi_palette) 
    plt.title(f'Distribución de {column} por Depresión')
    plt.tight_layout()
    plt.show()
```

-   Se observa una relación entre la variable de `age` y si el adulto tiene depresión o no, ya que se concentra más en edades menores de 40 años, los adultos que tienen depresión.

-   En cuanto a `cgpa`, se observa que tienen una distribución similar entre los adultos que tienen o no depresión.

-   Para work/study_hours,

```{python}
#| echo: false
value_counts = train['profession'].value_counts()
sizes = value_counts.values[:20]  # Mostramos el top 20
colors = sns.color_palette("pastel", len(sizes))

labels = [
    "Customer\nSupport" if label == "Customer Support" else
    "Educational\nConsultant" if label == "Educational Consultant" else
    "Travel\nConsultant" if label == "Travel Consultant" else
    "Data\nScientist" if label == "Data Scientist" else
    "Business\nAnalyst" if label == "Business Analyst" else
    "Marketing\nManager" if label == "Marketing Manager" else
    label
    for label in value_counts.index[:20]
]

plt.figure(figsize=(8, 6))
squarify.plot(sizes=sizes, 
              label=labels, 
              color=colors, 
              text_kwargs = {'fontsize': 8, 'color': 'black'},
              pad=True)
plt.title(f"Mapa de profesiones (Top 20)")
plt.axis("off")
plt.show()
```

-   En cuanto a ocupaciones, destaca una alta proporción de personas en el sector educativo, específicamente profesores, que constituyen la profesión predominante.

```{python}
#| echo: false
top_n_professions = 20
profession_counts = train['profession'].value_counts().nlargest(top_n_professions)
filtered_data = train[train['profession'].isin(profession_counts.index)]

sankey_data = filtered_data.groupby(['profession', 'depression']).size().reset_index(name='Count')

labels = list(sankey_data['profession'].unique()) + ['No Depression', 'Depression']
source_indices = []
target_indices = []

for _, row in sankey_data.iterrows():
    profession_index = labels.index(row['profession'])
    depression_index = labels.index('Depression' if row['depression'] == 1 else 'No Depression')
    source_indices.append(profession_index)
    target_indices.append(depression_index)

palette = sns.color_palette("pastel", len(labels))
colors = [f'rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, 0.8)' for r, g, b in palette]

fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels,
        color=colors),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=sankey_data['Count']
    ))])

fig.update_layout(
    title_text="Diagrama de Sankey de profesión y depresión", 
    font_size=10,
    width=700, 
    height=600)

fig.show()
```

## 3. Preprocesamiento de los datos

### 3.1 Ingenieria de variables

Se añadieron características derivadas para enriquecer el dataset y mejorar el análisis. Estas incluyen:

-   `satisfaction_by_work`: Relación entre presión laboral y satisfacción en el trabajo.
-   `satisfaction_by_study`: Relación entre presión y satisfacción académica.
-   `age_work_satisfaction`: Razón entre edad y satisfacción laboral.
-   `cgpa_study`: Relación entre promedio académico y presión académica.

Estas nuevas variables buscan capturar interacciones clave, como el impacto de la presión en la satisfacción o el desempeño bajo diferentes condiciones, y tienen el potencial de mejorar el modelo predictivo. Las varibales se crean con la siguiente función:

```{python}
#| label: new feats
def new_feats(df):
    df = (
        df.assign(satisfaction_by_work=df["work_pressure"] / df["job_satisfaction"])
        .assign(
            satisfaction_by_study=df["academic_pressure"] / df["study_satisfaction"]
        )
        .assign(age_work_satisfaction=df["age"] / df["job_satisfaction"])
        .assign(cgpa_study=df["cgpa"] / df["academic_pressure"])
    )
    return df

train = new_feats(train).copy()
test = new_feats(test).copy()
```

Adicionalmente, se realizó un proceso de limpieza de datos enfocado en la estandarización de los valores de la columna `degree` en la base de datos. Para ello, estoy utilizando un diccionario de mapeo que normaliza y unifica las diferentes representaciones de títulos académicos. Este mapeo incluye equivalencias entre términos que, aunque diferentes en la base de datos, representan el mismo grado académico.

Por ejemplo:

-   Variaciones como `"BCom"`, `"B.Com"`, y `"B.Comm"` se unifican bajo el valor estándar `"B.Com"`.

-   Representaciones como `"MSc"`, `"M.Sc"`, y `"Master of Science"` se convierten en `"M.Sc"`.

-   Títulos escolares como `"Class 12"` y `"12th"` se consolidan en `"Class 12"`.

El propósito de este procedimiento es garantizar la consistencia en los valores de la columna, facilitando análisis posteriores y mejorando la calidad del modelo al trabajar con datos limpios y estandarizados. La misma transformación se aplica tanto al conjunto de entrenamiento como al de prueba para mantener la uniformidad entre ambos datasets.

```{python}
#| echo: false
degree = {
    "BCom": "B.Com", "B.Com": "B.Com", "B.Comm": "B.Com",
    "B.Tech": "B.Tech", "BTech": "B.Tech", "B.T": "B.Tech",
    "BSc": "B.Sc", "B.Sc": "B.Sc", "Bachelor of Science": "B.Sc",
    "BArch": "B.Arch", "B.Arch": "B.Arch",
    "BA": "B.A", "B.A": "B.A",
    "BBA": "BBA", "BB": "BBA",
    "BCA": "BCA",
    "BE": "BE",
    "BEd": "B.Ed", "B.Ed": "B.Ed",
    "BPharm": "B.Pharm", "B.Pharm": "B.Pharm",
    "BHM": "BHM",
    "LLB": "LLB", "LL B": "LLB", "LL BA": "LLB", "LL.Com": "LLB", "LLCom": "LLB",
    "MCom": "M.Com", "M.Com": "M.Com",
    "M.Tech": "M.Tech", "MTech": "M.Tech", "M.T": "M.Tech",
    "MSc": "M.Sc", "M.Sc": "M.Sc", "Master of Science": "M.Sc",
    "MBA": "MBA",
    "MCA": "MCA",
    "MD": "MD",
    "ME": "ME",
    "MEd": "M.Ed", "M.Ed": "M.Ed",
    "MArch": "M.Arch", "M.Arch": "M.Arch",
    "MPharm": "M.Pharm", "M.Pharm": "M.Pharm",
    "MA": "MA", "M.A": "MA",
    "MPA": "MPA",
    "LLM": "LLM",
    "PhD": "PhD",
    "MBBS": "MBBS",
    "CA": "CA",
    "Class 12": "Class 12", "12th": "Class 12",
    "Class 11": "Class 11", "11th": "Class 11"
}

train['degree'] = train['degree'].map(degree)
test['degree'] = test['degree'].map(degree)
```

Dado que algunas variables categóricas presentan un gran número de categorías únicas, se implementó una estrategia para agrupar las categorías menos representativas en una etiqueta denominada "*Other*". Esto asegura que las categorías seleccionadas representen al menos el 90% de los datos, reduciendo la dimensionalidad y facilitando el análisis.

Las columnas ajustadas incluyen:

-   `city`
-   `profession`
-   `sleep_duration`
-   `dietary_habits`
-   `degree`

Esta transformación permite simplificar la estructura del dataset sin perder información relevante, mejorando la manejabilidad del modelo y reduciendo el riesgo de sobreajuste debido a categorías con baja frecuencia.

```{python}
#| echo: false
def group_low_frequency(train, test, column, threshold_percentage):
    """
    Agrupa las categorías con representación acumulativa menor al umbral definido en "Other".

    Args:
        train (pd.DataFrame): DataFrame de entrenamiento.
        test (pd.DataFrame): DataFrame de prueba.
        column (str): Columna objetivo a transformar.
        threshold_percentage (float): Umbral de porcentaje (0-100) para incluir categorías acumulativas.

    Returns:
        pd.Series, pd.Series: Columnas transformadas para `train` y `test`.
    """
    value_counts = train[column].value_counts(normalize=True).sort_values(ascending=False)
    cumsum = value_counts.cumsum()
    valid_categories = cumsum[cumsum <= threshold_percentage].index
    train_transformed = train[column].apply(lambda x: x if x in valid_categories else "Other")
    test_transformed = test[column].apply(lambda x: x if x in valid_categories else "Other")
    return train_transformed, test_transformed

```

```{python}

train["city"], test["city"] = group_low_frequency(train, test, 'city', 0.9)
train["profession"], test["profession"] = group_low_frequency(train, test, 'profession', 0.9)
train["sleep_duration"], test["sleep_duration"] = group_low_frequency(train, test, 'sleep_duration', 0.9)
train["dietary_habits"], test["dietary_habits"] = group_low_frequency(train, test, 'dietary_habits', 0.9)
train["degree"], test["degree"] = group_low_frequency(train, test, 'degree', 0.9)
```

Tras la agrupación, las variables categóricas principales quedan resumidas con las categorías más representativas

```{python}
#| echo: false
train[["city","profession","sleep_duration","dietary_habits","degree"]].describe(include="object")
```

### 3.2 Relación variables categóricas y depresión

```{python}
#| echo: false
#| label: catplt
def catplt(x_var, aspect = 4):
    gs = sns.catplot(
        data=train,
        x=x_var,
        kind="count",
        hue="depression",
        height=2,
        aspect=aspect,
        sharey=False,
        palette=target_colors,
        legend_out=False,
        legend="auto",
        width=0.5,
    )
    for ax in gs.axes.ravel():
        for i in ax.containers:
            ax.bar_label(i, label_type="edge", fontsize=8)
            ax.set_xlabel("", fontsize=12)
            ax.set_ylabel("", fontsize=12)
            ax.tick_params(axis="both", labelsize=8)
            gs._legend.remove()
            gs.set_titles(size=8)

    plt.legend(title="Target", title_fontsize=8, fontsize=8, loc="best")
    plt.show()
```

```{python}
#| echo: false
catplt(x_var="work/study_hours", aspect= 8)
```

-   El análisis gráfico de la variable `work/study_hours` muestra un patrón interesante, ya que a medida que aumentan las horas dedicadas al trabajo o estudio, también aumenta la proporción de casos con indicadores de depresión.

```{python}
#| echo: false
catplt(x_var="working_professional_or_student")
```

-   Los resultados muestran que más de la mitad de los estudiantes en el dataset tienen indicadores de depresión. Esto resalta una diferencia notable en comparación con los profesionales, sugiriendo que el entorno académico puede ser un factor de estrés significativo.

```{python}
#| echo: false
catplt(x_var="gender")
```

-   Aunque hay más hombres con depresión en términos absolutos, los porcentajes de depresión entre hombres y mujeres son similares, alrededor del 18%. Esto indica que la distribución de la variable objetivo no presenta un sesgo significativo entre géneros.

```{python}
#| echo: false
catplt(x_var="study_satisfaction")
```

-   Se observa una clara relación inversa entre la satisfacción en los estudios y la propensión a la depresión. A menor satisfacción académica, mayor es la proporción de casos positivos de depresión, lo que refuerza la importancia del bienestar en el ámbito educativo.

### 3.3 Mapa de correlación entre variables numéricas

```{python}
#| echo: false
plt.style.use("default")
corr_mat = train[num_feats].corr()
mask = np.triu(np.ones_like(corr_mat, dtype=bool))
cmap = sns.diverging_palette(230, 30, as_cmap=True)
f, ax = plt.subplots(figsize=(9, 7))
sns.heatmap(
    corr_mat,
    mask=mask,
    cbar=True,
    cmap=cmap,
    center=0,
    square=False,
    annot=False,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8},
)
plt.title("Matriz de correlaciones\n")
plt.xticks(fontsize=9)
plt.yticks(fontsize=9)
plt.show()
```

## 4. Algoritmos y modelos

```{python}
#| echo: false
#| label: Sklearn libraries
from sklearn.model_selection import (
    train_test_split,
    StratifiedKFold)

from catboost import CatBoostClassifier, Pool
from sklearn.linear_model import LogisticRegression
```

### 4.1 Regresión Logística

La regresión logística, es un modelo estadístico ampliamente utilizado en aprendizaje automático para problemas de clasificación binaria y multiclase. Este modelo se basa en la relación entre una o más variables independientes y una variable dependiente categórica, utilizando una función sigmoide para estimar la probabilidad de pertenencia a una clase específica. La simplicidad y la interpretabilidad de la regresión logística lo convierten en una buena opción para problemas de clasificación, especialmente cuando el objetivo es comprender la relación entre las variables predictoras y la variable objetivo.

Para entrenar el modelo de regresión logística, se manejaron los valores faltantes en el conjunto de datos reemplazándolos con "0". Posteriormente, se seleccionaron las columnas relevantes (pair_cols) para el análisis, asegurando que tanto las variables predictoras como la variable objetivo (`depression`) estuvieran correctamente estructuradas.

El modelo de regresión logística se configuró con un número máximo de iteraciones de 10,000 y se entrenó sobre el conjunto de entrenamiento. Después de ajustar el modelo, su desempeño fue evaluado en el conjunto de prueba, obteniendo una precisión (*accuracy*) de 0.9185. Este resultado establece un punto de referencia (*baseline*) para comparar el desempeño de modelos más complejos.

La regresión logística, con su simplicidad y rapidez, sirve como un modelo inicial para entender la estructura de los datos y evaluar si enfoques más avanzados pueden mejorar significativamente el desempeño en la predicción de la variable objetivo.

```{python}
#| echo: false
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
pair_cols = num_feats.append(target)
logi_train = train.fillna("0")
logi_test = test.fillna("0")

logi_train = logi_train[pair_cols]
logit_num_feats_test = pair_cols[:-1]
logi_test = logi_test[logit_num_feats_test]

logi_y = logi_train["depression"]
logi_X = logi_train.drop(["depression"], axis=1)
logi_X_train, logi_X_test, logi_y_train, logi_y_test = train_test_split(
    logi_X, logi_y, test_size=0.3, random_state=42)
logi_reg = LogisticRegression(max_iter = 10000)
logi_reg.fit(logi_X_train, logi_y_train)
logi_y_pred = logi_reg.predict(logi_X_test)
print('Accuracy of logistic regression classifier on validation set: {:.6f}'.format(logi_reg.score(logi_X_test, logi_y_test)))
```

### 4.2 XGBClassifier

El **XGBClassifier**, parte de la biblioteca XGBoost (eXtreme Gradient Boosting), un potente algoritmo de aprendizaje automático basado en árboles de decisión, diseñado específicamente para tareas de clasificación. Este modelo es conocido por su capacidad para manejar grandes volúmenes de datos, trabajar con combinaciones complejas de variables numéricas y categóricas, y optimizar tanto el tiempo de entrenamiento como el rendimiento del modelo. Incluye funcionalidades avanzadas como regularización integrada para reducir el riesgo de sobreajuste y soporte para paralelización, lo que lo hace especialmente eficiente y robusto en problemas de clasificación binaria o multiclase.

Elegimos **XGBClassifier** para este ejercicio porque, además de su capacidad para manejar estructuras complejas, proporciona configuraciones específicas para clasificación, como la función de pérdida `log loss` optimizada para este tipo de problemas. Esto es particularmente útil en nuestro conjunto de datos, que combina variables numéricas y categóricas con una posible presencia de interacciones no lineales.

Nuestro objetivo es comparar el desempeño de **XGBClassifier** frente al modelo baseline de Regresión Logística, evaluando si el uso de un modelo más avanzado mejora significativamente la capacidad predictiva para la variable objetivo, **depression**.

Para entrenar este modelo, el conjunto de datos fue preprocesado de manera similar al modelo baseline, asegurando la limpieza y adecuación de las variables predictoras y objetivo. Se ajustaron hiperparámetros clave como `max_depth`, `learning_rate` y `n_estimators` para optimizar el balance entre el desempeño del modelo y el riesgo de sobreajuste. El desempeño del modelo fue evaluado utilizando métricas como precisión (**accuracy**), la curva ROC-AUC, y una matriz de confusión para visualizar la clasificación y los errores del modelo.

El **XGBClassifier** proporciona una representación más detallada de las relaciones no lineales en los datos y, al estar específicamente diseñado para clasificación, ofrece herramientas optimizadas para este propósito. Su rendimiento será comparado directamente con el modelo baseline para determinar su capacidad de predecir correctamente la presencia de depresión en los datasets analizados.

```{python}
#| eval: false
from sklearn.metrics import accuracy_score, roc_auc_score
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 

def objective(trial):
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1)
    n_estimators = trial.suggest_int('n_estimators', 600, 5000)
    learning_rate = trial.suggest_float('learning_rate', 0.07, 0.1)
    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)
    reg_alpha = trial.suggest_float('reg_alpha', 1, 3)
    max_depth = trial.suggest_int('max_depth', 8, 15)
    gamma = trial.suggest_float('gamma', 0.2, 0.5)
    eval_metric = 'auc'
    
    model = XGBClassifier(
        colsample_bytree=colsample_bytree,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        reg_alpha=reg_alpha,
        reg_lambda=reg_lambda,
        gamma=gamma,
        eval_metric=eval_metric,
        random_state=42)
    
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    roc_auc = roc_auc_score(y_test, y_proba)
    accuracy = accuracy_score(y_test, y_pred)
        
    return accuracy, roc_auc
```

```{python}
best_params_xgbc = {
'colsample_bytree': 0.6065523520125417, 
'n_estimators': 737, 
'learning_rate': 0.08955000476127656, 
'reg_lambda': 0.36852634372376114, 
'reg_alpha': 2.7287164996792743, 
'max_depth': 11, 
'gamma': 0.49045802837441654
}
```

```{python}
#| echo: false
import pandas as pd
general_metrics = pd.DataFrame({
    'Metric': ['Accuracy', 'ROC-AUC'],
    'Value': [0.939825, 0.975605]})

classification_metrics = pd.DataFrame.from_dict({
        'precision': [0.960072, 0.846847, 0.939825, 0.903459, 0.939213],
        'recall': [0.966429, 0.822016, 0.939825, 0.894222, 0.939825],
        'f1-score': [0.963240, 0.834247, 0.939825, 0.898743, 0.939476],
        'support': [34434.0, 7776.0, 42210.0, 42210.0, 42210.0]},
    orient='index',
    columns=['Class 0', 'Class 1', 'accuracy', 'macro avg', 'weighted avg'])
general_metrics
```

### 4.3 CatBoost

CatBoost es un algoritmo de aprendizaje automático basado en árboles de decisión que utiliza técnicas avanzadas de boosting. Es especialmente útil para trabajar con datos categóricos, ya que implementa un manejo eficiente de estas variables mediante codificación automática, lo que reduce la necesidad de preprocesamiento adicional. Además, CatBoost es conocido por su capacidad de minimizar el riesgo de sobreajuste y su alto desempeño en tareas de clasificación y regresión.

Elegimos CatBoost para este ejercicio debido a la naturaleza del dataset, que contiene una combinación de variables numéricas y categóricas. Su capacidad para procesar variables categóricas de forma nativa, junto con su robustez frente al desbalance de datos, lo hace ideal para predecir nuestra variable objetivo, **depression**, en un conjunto de datos con estructuras y distribuciones complejas. Además, el rendimiento competitivo de CatBoost en comparación con otros algoritmos de boosting lo convierte en una opción sólida para este tipo de problema.

Para entrenar el modelo, el dataset se preprocesó llenando valores faltantes con "0" y ajustando los tipos de datos según la naturaleza de las variables (numéricas, categóricas y objetivo). Posteriormente, se dividieron los datos en conjuntos de entrenamiento y prueba, utilizando el 70% de los datos para entrenamiento y el 30% para prueba. Este procedimiento garantiza una evaluación justa del modelo, preservando datos no vistos para validar su desempeño y evitar el sobreajuste.

```{python}
#| echo: false
#| label: CatBoost
cat_train = train.fillna("0")
cat_test = test.fillna("0")
cat_train[num_feats] = cat_train[num_feats].astype("float64")
cat_train[obj_feats] = cat_train[obj_feats].astype("string")
cat_train[target] = cat_train[target].astype("int64")
cat_test[num_feats] = cat_test[num_feats].astype("float64")
cat_test[obj_feats] = cat_test[obj_feats].astype("string")
y = cat_train["depression"]
X = cat_train.drop(["depression"], axis=1)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=4
)

print("----------X Shape---------")
print(X.shape)
print("*************************************")
print("----------y Shape------------")
print(y.shape)

```

En este paso, utilizamos Optuna, una biblioteca eficiente para la optimización de hiperparámetros, con el objetivo de encontrar la mejor configuración para el modelo de CatBoostClassifier. La función de optimización busca maximizar el desempeño del modelo en términos de precisión (accuracy) utilizando un enfoque basado en validación cruzada. Espacio de Búsqueda

El espacio de búsqueda incluye los siguientes hiperparámetros:

```{python}
def objective(trial):
    # Espacio de búsqueda para los hiperparámetros
    params = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'depth': trial.suggest_int('depth', 4, 10),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 20.0),
        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),
    }

    # Modelo de CatBoost
    model = CatBoostClassifier(**params, cat_features=list(obj_feats), verbose=False)

    # Evaluación usando validación cruzada
    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()

    return score
```

**Función Objetivo**

La función objetivo se define como el promedio de la precisión obtenida mediante validación cruzada con 3 particiones ($k=3$):

$$\text{Score} = \frac{1}{k} \sum_{i=1}^{k} \text{Accuracy}(i)$$

Donde

$\text{Accuracy}(i)$ es la precisión en el $i$-ésimo fold de la validación cruzada.

El modelo de CatBoost se entrena y evalúa en cada iteración con una combinación específica de hiperparámetros sugerida por Optuna.

```{python}
best_params = {'iterations': 633,
 'learning_rate': 0.18918646362839026,
 'depth': 4,
 'colsample_bylevel': 0.9909843814690833,
 'subsample': 0.9778634549619083,
 'min_data_in_leaf': 1,
 'l2_leaf_reg': 10.504885456201343,
 'random_strength': 0.664861942866057}
```

```{python}
#| eval: false
#| label: Catboost run
cat_interp = CatBoostClassifier(**best_params, verbose=True, cat_features=X[obj_feats].columns.values, early_stopping_rounds=200)
cat_cv_last = StratifiedKFold(20, shuffle=True, random_state=42)
cat_cv_splits = cat_cv_last.split(X, y)
acc_scores = []
last_test_df_preds = []
cat_features = X[obj_feats].columns.values
X[cat_features] = X[cat_features].fillna('nan').astype(str)
X_test_df_pool = Pool(test, cat_features=X[obj_feats].columns.values)
for i, (full_train_idx, valid_idx) in enumerate(cat_cv_splits):
    model_fold = CatBoostClassifier(**best_params)
    X_train_fold, X_valid_fold = X.loc[full_train_idx], X.loc[valid_idx]
    y_train_fold, y_valid_fold = y.loc[full_train_idx], y.loc[valid_idx]
    X_train_pool = Pool(X_train_fold, y_train_fold, cat_features=X[obj_feats].columns.values)
    X_valid_pool = Pool(X_valid_fold, y_valid_fold, cat_features=X[obj_feats].columns.values)
    model_fold.fit(X=X_train_pool, eval_set=X_valid_pool, verbose=False, early_stopping_rounds=100)
    valid_pred = model_fold.predict(X_valid_pool)
    score = accuracy_score(y_valid_fold, valid_pred)
    acc_scores.append(score)
    test_df_pred = model_fold.predict_proba(X_test_df_pool)[:, 1]
    last_test_df_preds.append(test_df_pred)
```

Fold 1 Accuracy Score: 0.9383

Fold 2 Accuracy Score: 0.9420

Fold 3 Accuracy Score: 0.9375

Fold 4 Accuracy Score: 0.9389

Fold 5 Accuracy Score: 0.9419

Fold 6 Accuracy Score: 0.9407

Fold 7 Accuracy Score: 0.9396

Fold 8 Accuracy Score: 0.9429

Fold 9 Accuracy Score: 0.9403

Fold 10 Accuracy Score: 0.9395

```{python}
#| eval: false
from sklearn.metrics import classification_report
print(classification_report(y_valid_fold, valid_pred))
```

```{python}
#| echo: false
import pandas as pd
metrica = pd.DataFrame({
    'Metric': ['Accuracy'],
    'Value': [0.9402061122956645]})

metrica
```

## 5. Selección de modelo

De acuerdo a la métrica que se seleccionó, el mejor modelo es un **CatBoost** el cual presentó una precisión media de 0.9402 en la validación cruzada. Este modelo supera a la **regresión logística** (modelo baseline) y al **XGBClassifier**, que obtuvieron una precisión de 0.9185 y 0.9398, respectivamente. A continuación se presentan los resultados obtenidos, así como la comparación entre modelos.

### 5.1 Optimización de hiperparámetros

La optimización de los hiperparámetros del modelo se llevó a cabo utilizando la biblioteca **Optuna**, una herramienta de optimización automática de hiperparámetros basada en técnicas de búsqueda bayesiana.

Por defecto, Optuna implementa un algoritmo de optimización bayesiano (TPE), pero se puede cambiar fácilmente a otros algoritmos existentes en el paquete.Optuna denomina sus algoritmos de optimización en dos categorías diferentes:

**Sampling strategy**: algoritmos que seleccionan la mejor combinación de parámetros, concentrándose en las zonas, en las que los hiperparámetros, dan mejores resultados.

**Pruning strategy**: Métodos de optimización basados en la parada temprana.

**TPESampler** (Tree-Structured Parzen Estimator), es un algoritmo de optimización bayesiano que:

-   En primer lugar, selecciona aleatoriamente un subconjunto de hiperparámetros y los ordena en función de sus puntuaciones.

-   Los hiperparámetros ,se dividen a su vez en dos grupos, en función de algún cuantil predefinido.

-   A continuación, los dos grupos se modelan en densidades estimadas $l(x_1)$ y $g(x_2)$ utilizando estimadores *Parzen* (estimadores de densidad kernel).

-   Localiza los hiperparámetros con mayor mejora esperada \[menor $l(x_1)$/ $g(x_2)$ \]. Los hiperparámetros con mayor mejora esperada (*expected improvement*) se evalúan, ordenan y dividen de nuevo.

-   Este proceso se repite hasta que finaliza el presupuesto y se devuelven los mejores hiperparámetros.

**SuccessiveHalvingPruner** (Asynchronous Successive Halving)

Selecciona aleatoriamente un conjunto inicial de valores de hiperparámetros. Entrena los ensayos durante 1 época, hasta alcanzar el número máximo de ensayos definido.

Al mismo tiempo, una prueba es promovida simultáneamente a otro escalón (similar al rango) para entrenar más épocas, siempre que la puntuación de la prueba esté entre el $d$ por ciento superior dentro del escalón donde $d$ es un divisor predefinido.

Tenga en cuenta que esto es diferente de la división a la mitad sucesiva síncrona, en la que el algoritmo espera a que todas las pruebas definidas en un escalón terminen sus épocas y sólo entonces decide qué pruebas tienen las mejores puntuaciones para ser promovidas a entrenar más épocas en otro escalón.

el enfoque descrito anteriormente, permite identificar de manera eficiente la combinación óptima de parámetros que maximicen el rendimiento del modelo, en este caso, el *accuracy*.

### Proceso de Optimización:

1.  **Definición de la Función Objetivo**: La función objetivo fue diseñada para entrenar el modelo **XGBoost** con un conjunto de hiperparámetros sugeridos en cada iteración y calcular el rendimiento del modelo en términos de la métrica (*accuracy*).

2.  **Espacio de Búsqueda**: Se definió un rango para cada hiperparámetro clave:

    -   `colsample_bytree`: Proporción de columnas utilizadas por árbol.

    -   `n_estimators`: Número de árboles.

    -   `learning_rate`: Tasa de aprendizaje.

    -   `reg_lambda`: Regularización L2.

    -   `reg_alpha`: Regularización L1.

    -   `max_depth`: Profundidad máxima del árbol.

    -   `gamma`: Reducción mínima de la pérdida necesaria para realizar una partición.

3.  **Optimización**: Se utilizó el método de búsqueda aleatoria dentro de **Optuna** para explorar el espacio de hiperparámetros. El proceso incluyó:

    -   **300 iteraciones** para evaluar diferentes combinaciones de parámetros.

    -   **Evaluación cruzada** mediante el conjunto de datos de entrenamiento y prueba.

    -   Uso de un **callback** para registrar los mejores resultados en tiempo real.

4.  **Resultados**:

    | **Hyperparameter** | **Value** | **Description** |
    |-----------------|-----------------|---------------------------------------|
    | **iterations** | 633 | Número total de iteraciones del modelo. |
    | **learning_rate** | 0.18918646362839026 | Tasa de aprendizaje para ajustar la contribución de cada iteración. |
    | **depth** | 4 | Profundidad máxima de los árboles de decisión. |
    | **colsample_bylevel** | 0.9909843814690833 | Proporción de columnas seleccionadas para cada nivel del árbol. |
    | **subsample** | 0.9778634549619083 | Proporción de muestras seleccionadas para entrenar cada iteración. |
    | **min_data_in_leaf** | 1 | Número mínimo de observaciones necesarias en una hoja. |
    | **l2_leaf_reg** | 10.504885456201343 | Regularización L2 aplicada a las hojas para prevenir sobreajuste. |
    | **random_strength** | 0.664861942866057 | Amplitud de la distribución aleatoria utilizada para las divisiones del árbol. |

Estos hiperparámetros optimizados permiten al modelo de CatBoost alcanzar un desempeño óptimo en la predicción de la variable objetivo, **depression**, en el dataset analizado. La optimización de hiperparámetros es un paso crítico en el desarrollo de modelos de aprendizaje automático, ya que permite ajustar la configuración del modelo para maximizar su capacidad predictiva y generalización.

### 5.2 Validación cruzada

El modelo de CatBoost se evaluó mediante validación cruzada con 20 particiones estratificadas, lo que garantiza una distribución equitativa de las clases en cada fold. Los resultados de la validación cruzada muestran una precisión promedio de 0.9402 en los 10 folds, lo que indica un desempeño consistente y robusto del modelo en la predicción de la variable objetivo, **depression**. La precisión obtenida en cada fold es la siguiente:

| **Fold**    | **Accuracy Score**    |
|-------------|-----------------------|
| Fold 1      | 0.9383084577114428    |
| Fold 2      | 0.9420753375977257    |
| Fold 3      | 0.9375977256574272    |
| Fold 4      | 0.9389481165600568    |
| Fold 5      | 0.9419331911869225    |
| Fold 6      | 0.940724946695096     |
| Fold 7      | 0.9396588486140725    |
| Fold 8      | 0.9429282160625444    |
| Fold 9      | 0.9403695806680882    |
| Fold 10     | 0.9395167022032693    |
| **Average** | **0.940606312395064** |

### 5.3 Comparación de modelos

Los modelos de CatBoost, XGBClassifier y Regresión Logística se compararon en términos de precisión (accuracy) y otras métricas relevantes para evaluar su desempeño en la predicción de la variable objetivo, **depression**. Los resultados obtenidos se presentan a continuación:

| **Model**           | **Accuracy** |
|---------------------|--------------|
| Regresión Logística | 0.9185       |
| XGBClassifier       | 0.9398       |
| CatBoost            | 0.9402       |

## 6. Conclusiones

En este reporte, exploramos la relación entre diversos factores y la presencia de depresión en adultos mediante técnicas de aprendizaje automático. La depresión, como condición compleja que afecta aspectos emocionales, cognitivos y físicos, requiere un enfoque que permita identificar patrones subyacentes en los datos. A través de modelos como la **Regresión Logística**, el **XGBClassifier** y el **CatBoost**, se analizaron estas relaciones y se evaluó la capacidad predictiva de cada enfoque.

El modelo de **Regresión Logística**, usado como baseline, permitió identificar relaciones lineales entre variables como satisfacción laboral, horas de sueño, y presión académica con la depresión. Su simplicidad y alta interpretabilidad facilitaron una comprensión inicial de cómo cada variable contribuye a predecir la variable objetivo. Sin embargo, su desempeño quedó limitado al no capturar interacciones más complejas entre los factores.

El **XGBClassifier** representó un avance significativo al modelar relaciones no lineales y combinaciones entre variables, como la interacción entre estrés financiero, historial familiar de enfermedades mentales, y satisfacción general. Este modelo ofreció una mayor precisión global y destacó factores clave con mayor peso en la clasificación.

Sin embargo, fue el modelo de **CatBoost** el que demostró ser el más robusto y eficiente en la predicción de la depresión. CatBoost, gracias a su manejo nativo de variables categóricas y su capacidad para regularizar el sobreajuste, obtuvo el mejor desempeño general. Este modelo sobresalió no solo en métricas de evaluación como precisión y AUC-ROC, sino también en su capacidad de interpretabilidad al identificar las variables más influyentes. Las relaciones detectadas entre variables como satisfacción académica, presión laboral, y horas de sueño reflejan un enfoque integral que permite abordar la depresión desde múltiples dimensiones. Este análisis demuestra que el aprendizaje automático, combinado con enfoques avanzados como CatBoost, es una herramienta poderosa para predecir condiciones complejas como la depresión. Las relaciones detectadas entre variables no solo mejoran la capacidad de predicción, sino que también ofrecen insights valiosos para diseñar estrategias preventivas y personalizadas. Al integrar estos modelos en entornos clínicos y de bienestar, es posible mejorar la detección temprana y el tratamiento de la depresión, contribuyendo a una atención más efectiva y centrada en el paciente.

## 7. Referencias

-   Organización Mundial de la Salud (OMS). (2023). Depression. Recuperado de <https://www.who.int/es/news-room/fact-sheets/detail/depression>
-   Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). *Optuna: A Next-generation Hyperparameter Optimization Framework*. arXiv. <https://arxiv.org/abs/1907.10902>
-   Hastie, T., Tibshirani, R., & Friedman, J. (2017). *The elements of statistical learning: Data mining, inference, and prediction* (2nd ed.). Springer. [https://doi.org/10.1007/978-0-387-84858-7](https://hastie.su.domains/ElemStatLearn/)
-   Kaggle. (n.d.). *Exploring Mental Health Data: Playground Series - Season 4, Episode 11*. <https://www.kaggle.com/competitions/playground-series-s4e11>
-   Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press. Retrieved from <http://www.deeplearningbook.org>
-   Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press. <http://www.deeplearningbook.org>
